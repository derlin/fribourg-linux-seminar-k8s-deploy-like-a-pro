{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>Want to learn Kubernetes and how to deploy a containerized application on it? You're in the right place!</p> <p>Join me for a journey starting on the basics of Kubernetes, including pods, deployments, and manifests. I'll then show you how to package your app for the cloud with Helm. And because a single app doesn't do much, we'll turn to helmfile for assistance. Lastly, I'll mention the importance of GitOps, and demonstrate how Argo CD can be used to automate deployments and streamline changes.</p> <p>Ready? Let's get started!</p> <p>Important</p> <p>This website/tutorial has been set up for my presentation at the 24th Fribourg Linux Seminar: Kubernetes and Friends hosted in Fribourg on May 11th, 2023 (archives). It may be outdated by the time you read it. Find the versions used later in this page.</p> <p>Recording of the talk (in French):</p> <p> </p>"},{"location":"#kubernetes","title":"Kubernetes","text":"<p>In a single sentence:</p> <p>Quote</p> <p>Kubernetes - or its numeronym k8s - is an open-source container orchestration platform that automates the deployment, scaling, and management of containerized applications.</p> <p>Developed first internally by Google, it was open-sourced in 2014 and became one of the pillars of the Cloud Native Computing Foundation. It is now ubiquitous, and supported by all major cloud providers. It can also run locally on Docker using minikube or k3d!</p> Major managed Kubernetes providers <ol> <li>Amazon Web Services (AWS): Amazon Elastic Kubernetes Service (EKS)</li> <li>Microsoft Azure: Azure Kubernetes Service (AKS)</li> <li>Google Cloud Platform (GCP): Google Kubernetes Engine (GKE)</li> <li>IBM Cloud: IBM Cloud Kubernetes Service</li> <li>DigitalOcean: DigitalOcean Kubernetes</li> <li>Oracle Cloud Infrastructure (OCI): Oracle Kubernetes Engine (OKE)</li> <li>Alibaba Cloud: Alibaba Cloud Container Service for Kubernetes (ACK)</li> <li>Red Hat OpenShift: OpenShift Kubernetes Service (OKS)</li> <li>Exoscale: Scalable Kubernetes Service (SKS)</li> </ol> <p>The vastness and complexity of k8s is too much to take in one go. So in this pages, we will focus only on one aspect: how to deploy an application on Kubernetes.</p>"},{"location":"#requirements","title":"Requirements","text":"<p>Here are the tools we will use, and the versions I played with:</p> <ul> <li>kubectl: v1.25.1</li> <li>Terraform: v1.4.5</li> <li>Helm: v3.11.2</li> <li>Helmfile: v0.152.0</li> <li>Argo CD: v2.6.7</li> </ul>"},{"location":"#our-toy-application","title":"Our toy application","text":"<p>The application we will use is rickroller. It is a very simple webapp coded in Python Flask, that allows you to rickroll your friends. How it works it simple:</p> <ol> <li>The user enters an URL,</li> <li>Rickroller fetches the HTML content of the page, and modify it a bit so that:<ul> <li>all links redirect to a \"you got rickrolled page\"</li> <li>(optional) the redirect also happens after a given number of scrolls</li> </ul> </li> <li>Rickroller serves the resulting HTML back to the user</li> <li>The user can copy the URL of 3 and send it to his friends, waiting for them to get surprised.</li> </ol> <p>Test it!</p> <p>A live demo is available at \u2b95 https://tinyurl.eu.aldryn.io</p> <p>For the curious, it is deployed by Divio, which is awesome . Check it out!</p> <p>By default, the URL generated in 3 will use the hash of the original URL, which can be quite long (and thus suspicious). This is why rickroller also supports an URL shortener mode. For this, it needs a persistence layer to store the tuples slugs/URLs. Supported persistences are SQL (SQLite, PostgreSQL, MySQL, MariaDB, ...) and MongoDB.</p> <p>This means you can deploy rickroller using one or two images: rickroller, and the persistence. Here is how it looks with a docker-compose:</p> <pre><code>services:\nweb:\nimage: derlin/rickroller:latest\nports: [8080:8080]\nenvironment:\nDATABASE_URL: postgres://postgres@db:5432/db\nlinks: [db]\ndepends_on: [db]\n\ndb:\nimage: postgres:13.5-alpine\nenvironment:\nPOSTGRES_DB: db\nPOSTGRES_HOST_AUTH_METHOD: trust\n</code></pre> <p>Easy, right? Now, let's see how to run this same application in Kubernetes!</p>"},{"location":"00-sks/","title":"The Cluster","text":"<p>We first need a Kubernetes cluster. As mentioned in the introduction, there are lots available. Instead of running locally (using e.g. k3d), let's use the Scalable Kubernetes Service (SKS) from Exoscale, using the starter pricing tier.</p> <p>For the time being, consider \"pod\" a synonym for a container. We will see later the difference between regular containers and k8s pods. </p> Using a local Kubernetes cluster <p>If you do not have an account on Exoscale, or simply do not want to use a managed cluster, you can run everything on your machine using Kubernetes in Docker. For more information, head to the Want to work locally? section on this page.</p>"},{"location":"00-sks/#spawn-an-sks-cluster","title":"Spawn an SKS cluster","text":"<p>You'll find a terraform module in this repository that you can use to spin up your own. The only thing you need is a valid API key and secret.</p> <p>Login to Exoscale, navigate to IAM &gt; API KEY and create a new key. On your terminal, do: <pre><code>TF_VAR_exoscale_api_key=&lt;your key&gt;\nTF_VAR_exoscale_api_secret=&lt;your secret&gt;\n</code></pre></p> <p>Now you are ready. Navigate to <code>terraform</code> and run: <pre><code>terraform init\nterraform plan\nterraform apply -auto-approve\n</code></pre></p> <p>Wait a few minutes, and boom! You have a Kubernetes cluster. Copy and paste the output in your terminal, something like: <pre><code>export KUBECONFIG=...; kubectl cluster-info\n</code></pre></p> <p>Finally, install the Nginx Ingress Controller by running: <pre><code>## Ensure you exported KUBECONFIG=... first!\nkubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/exoscale/deploy.yaml\n</code></pre></p> <p>(The Nginx Ingress Controller was initially installed in terraform - see <code>terraform/main.tf</code>, but the destroy often fails, so better to do it manually.)</p> <p>Don't forget to stop the cluster</p> <p>Once you finished experimenting, don't forget to destroy your cluster to avoid unecessary costs: <pre><code># Uninstall Nginx Ingress Controller first,\n# or the load balancer on exoscale will stay there and cost you\nkubectl destroy -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/exoscale/deploy.yaml\n# Then destroy the cluster\nterraform destroy -auto-approve\n</code></pre> Always go to the Exoscale Console and ensure there is no remaining Load Balancers. You never know, and load balancers are expensive!</p>"},{"location":"00-sks/#what-composes-a-kubernetes-cluster","title":"What composes a Kubernetes cluster?","text":"<p>Kubernetes is a distributed system that consists of two main components: the control plane and the nodes. You communicate with the API server running in the control plane, and it takes the necessary action to make your workload run on some worker node(s).</p> <p>Need an analogy? Checkout Demystifying the Nuts &amp; Bolts of Kubernetes Architecture and What is Kubernetes? Confused? Kubernetes and its Architecture Explained in Plain English</p> <p></p> <p>The control plane is responsible for managing and orchestrating the various workloads that run on the nodes. It typically runs on a dedicated set of machines (separated from the worker nodes) and includes several key components:</p> API Server (<code>kube-apiserver</code>) The API server acts as the control plane's frontend, providing a RESTful API for users and other components to interact with. etcd etcd is a distributed key-value store that stores the cluster's configuration data and provides a reliable storage layer for the control plane. As you will see, k8s is declarative. All the states of the nodes, workloads, etc (in YAML or JSON format) are stored in etcd. Scheduler (<code>kube-scheduler</code>) The scheduler is responsible for determining which nodes in the cluster should run which pod. It takes into account many things such as the resource needs of the pods, such as CPU or memory, and the health of the cluster to make a decision.  Controller Manager (<code>kube-controller-manager</code>) The controller manager is responsible for running the core controllers that regulate the state of the cluster: Node controller (nodes down or added), Job controller, EndpointSlice controller, ServiceAccount controller, etc. Logically, each controller is a separate process, but to reduce complexity, they are all compiled into a single binary and run in a single process. (optional) Cluster Controller Manager (<code>cloud-controller-manager</code>) The cluster controller manager is only present when running a managed Kubernetes in the cloud (vs on-prem). It embeds cloud-specific logic specific to your cloud provider e.g. to manage routes, load balancers, nodes, etc. <p>The nodes, on the other hand, are the actual worker machines that run the containers and workloads  and are managed by the control plane (communicating through the API server). A node includes the following components:</p> Container runtime The software responsible for running containers. It can be any implementation of the Kubernetes CRI (Container Runtime Interface), but most usually containerd or CRI-O. <code>kubelet</code> kubelet is the agent responsible for managing Kubernetes pods on the machine, ensuring they are running and healthy. It also constantly communicates with the control plane about the status of the nodes and the containers running on it. <code>kube-proxy</code> kube-proxy maintains network rules on nodes, to allow network communication to pods from network sessions inside or outside the cluster. It typically relies on <code>iptables</code> (if available). <p>In the most minimal setup (e.g. k3d), one control plane and one node can run on a single machine. For a minimal HA - high availability - cluster, one would rather have three machines: one control plane and two workers. More nodes can be added to sustain the workload, and it is recommended to also make the control plane fault tolerant by scaling it horizontally on multiple machines.</p>"},{"location":"00-sks/#what-was-installed","title":"What was installed?","text":"<p>When creating the SKS cluster above, we created:</p> <ul> <li>a Kubernetes control plane (with an Exoscale Cloud Controller Manager),</li> <li>an Exoscale nodepool, which provisioned two compute instances that were registered as Kubernetes nodes (or worker nodes).</li> </ul> <p>Additionally, Exoscale's SKS automatically installs:</p> <ul> <li>The CNI1 plugin Calico: to manage the cluster network and add support for network policies,   security, and better encapsulation;   see Kubernetes CNI Explained.</li> <li>CoreDNS: a DNS server that provides service discovery and name resolution for Kubernetes services and pods,   allowing them to communicate with each other using human-readable domain names;</li> <li>Konnectivity: a tool that enables secure,   in-cluster communication between nodes and pods across different Kubernetes clusters,   providing a way to establish encrypted and authenticated connections over the public internet;</li> <li>Metrics server: an agent that collects resource metrics   from Kubelets and exposes them through the Kubernetes API server (<code>kubectl top</code>).</li> </ul> <p>But this is not all! The terraform script also potentially adds (see comments in <code>terraform/main.tf</code>):</p> <ul> <li>The NGINX Ingress Controller: to provide extrernal   access and manage routes (i.e. <code>Ingress</code>) based on hosts, path prefix, etc.</li> <li>Longhorn: a distributed block storage system that provides reliable and scalable persistent storage   for stateful applications.</li> </ul>"},{"location":"00-sks/#discover-your-cluster","title":"Discover your cluster","text":"<p>After exporting the <code>KUBECONFIG</code> environment variable, you can use <code>kubectl</code> to discover and interact with your cluster. kubectl is using HTTP requests to the API Server under the hood.</p> <p>Get general information: <pre><code>kubectl cluster-info\n</code></pre> <pre><code>Kubernetes control plane is running at https://xxxx.sks-ch-gva-2.exo.io:443\nCoreDNS is running at https://xxxx.sks-ch-gva-2.exo.io:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\n</code></pre></p> <p>Get the list of nodes: <pre><code>kubectl get nodes\n</code></pre> <pre><code>NAME               STATUS   ROLES    AGE    VERSION\npool-cb5ee-stszx   Ready    &lt;none&gt;   147m   v1.26.3\npool-cb5ee-uqiut   Ready    &lt;none&gt;   147m   v1.26.3\n</code></pre></p> <p>Note that above, only the worker nodes are shown (no role). This is because the control plane nodes are managed by Exoscale and thus hidden. If you were to execute this command on a local k3d cluster, you would see a single node with roles <code>control-plane,master</code>.</p> <p>See the nodes running in the kube namespace: <pre><code>kubectl get pod -n kube-system\n</code></pre> <pre><code>NAME                                       READY   STATUS    RESTARTS   AGE\ncalico-kube-controllers-5f94594857-dcbjh   1/1     Running   0          150m\ncalico-node-66kpj                          1/1     Running   0          149m\ncalico-node-jfbcj                          1/1     Running   0          149m\ncoredns-7cd5b7d6b4-c82vx                   1/1     Running   0          150m\ncoredns-7cd5b7d6b4-vbdns                   1/1     Running   0          150m\nkonnectivity-agent-799d489b4f-6gt5b        1/1     Running   0          150m\nkonnectivity-agent-799d489b4f-pdpls        1/1     Running   0          150m\nkube-proxy-njsgs                           1/1     Running   0          149m\nkube-proxy-smfdm                           1/1     Running   0          149m\nmetrics-server-77cc46c76-mlwkn             1/1     Running   0          149m\n</code></pre></p> <p>We have two worker nodes, thus two instances of most services: one for each!</p> <p>Finally, have a look at the metrics exposed by the Metrics server: <pre><code>kubectl top node # or top pod\n</code></pre> <pre><code>NAME               CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   \npool-cb5ee-stszx   89m          4%     1094Mi          65%       \npool-cb5ee-uqiut   121m         6%     1402Mi          83%  </code></pre></p>"},{"location":"00-sks/#use-a-ui","title":"Use a UI","text":"<p>Using <code>kubectl</code> may be tiring. Let's use a UI!</p> <p>There are lots of dashboards available. Here are my two personal favorites.</p> <p>k9s: A terminal-based UI.</p> <p></p> <p>Lens: a Kubernetes dashboard and IDE running on your local machine. Once installed, go to Files &gt; Add cluster and copy the content of your kubeconfig inside.  </p> <p></p>"},{"location":"00-sks/#want-to-work-locally","title":"Want to work locally?","text":"<p>Install k3d, a lightweight wrapper to run k3s (Rancher Lab\u2019s minimal Kubernetes distribution) in docker.</p> <p>Once done, you can create a cluster using <code>k3d cluster create</code>. By default, k3d installs a cluster with the traefik ingress controller. Since we are using the Nginx ingress controller in this demo, we need to disable traefik and install nginx.</p> <p>Create a cluster: <pre><code>k3d cluster create nginx \\\n--k3s-arg \"--disable=traefik@server:0\" \\\n--api-port 6550 -p \"8081:80@loadbalancer\"\n</code></pre></p> <p>In the command above, we:</p> <ul> <li>give a prefix to the name of the cluster - <code>nginx</code> - so the cluster name will be <code>k3d-nginx</code> (try running <code>kubectx</code>),</li> <li>ask k3d to map the API Server port to the port <code>6550</code> on our local machine (the host),</li> <li>ask k3d to map the Load Balancer port to the port <code>8081</code> of our local machine (remember everything runs on Docker!).</li> </ul> <p>You can of course change any of those values.</p> <p>Next, install the Nginx ingress controller: <pre><code>kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/provider/cloud/deploy.yaml\n</code></pre></p> <p>You are good to go! You can now follow along. Note that you will see some differences in the outputs, but not that many. Just remember:</p> <ul> <li>your k8s cluster only has one node, which is both a control plane node and a worker node,</li> <li>to access your load balancer, use http://localhost:8081 (or any port you supplied during the cluster creation).   It means you will have to access the rickroller app using http://localhost:8081/rickroller   when we will be talking about ingresses.</li> </ul> <ol> <li> <p>CNI, Container Network Interface \u21a9</p> </li> </ol>"},{"location":"01-deploy-raw/","title":"Manifests","text":"<p>First, let's deploy our rickroller application the \"manual\" way.</p>"},{"location":"01-deploy-raw/#namespaces","title":"Namespaces","text":"<p>Kubernetes comes with namespaces, a way to partition resources within a cluster into virtual clusters, or groups. Each namespace can have different policies, access controls, quotas etc. and are by default a bit isolated. They also provide a scope for names: names of resources need to be unique within a namespace, but not across namespaces. See Kubernetes namespaces isolation - what it is, what it isn't, life, universe and everything.</p> <p>If you don't specify a namespace, kubectl will connect to the default namespace idiomatically named <code>default</code>. You can easily create a namespace using <code>kubectl create namespace &lt;name&gt;</code>, and target it using the global <code>-n &lt;name&gt;</code> flag.</p> <p>To make a namespace the default for a session, you can try playing with kubectl contexts, or more conveniently install <code>kubectx</code> + <code>kubens</code>.</p> <pre><code># create a namespace called test\nkubectl create namespace test\n# make it the default for all subsequent commands (that do not use -n)\nkubens test\n</code></pre>"},{"location":"01-deploy-raw/#deploy-in-a-pod","title":"Deploy in a pod","text":"<p>The simplest way to get our Docker image running in k8s is to create a pod.</p> <p>Either run <code>kubectl run rickroller --image=derlin/rickroller:latest --port 8080</code>, or create the following YAML and apply it using <code>kubectl apply -f</code> (both are equivalent):</p> kube/pod.yaml<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\nname: rickroller\nspec:\ncontainers:\n- name: rickroller\nimage: derlin/rickroller\nports:\n- containerPort: 8080\n</code></pre> <p>Ensure it is running using: <code>kubectl get pod</code>.  How can we access it? The easiest way is to create a port forward:</p> <pre><code>kubectl port-forward pod/rickroller 8888:8080\n</code></pre> <p>You can now navigate to http://localhost:8888.</p>"},{"location":"01-deploy-raw/#what-are-pods-exactly","title":"What are pods exactly?","text":"<p>Pods are the smallest deployable unit on Kubernetes and the core of everything.</p> <p>A Pod is a self-sufficient higher-level construct. It is rather like a container, with a big difference: it can have multiple containers. All pod's containers run on the same machine (cluster node), their lifecycle is synchronized, and mutual isolation is weakened to simplify the inter-container communication.</p> A deeper look <p></p> <p>A Docker container is implemented using cgroups and namespaces. Usually, containers are isolated using the following namespaces:</p> <ul> <li><code>mnt</code> (Mount) - the container has an isolated mount table.</li> <li><code>uts</code> (UNIX Time-Sharing) - the container can have its own hostname and domain name.</li> <li><code>ipc</code> (Interprocess Communication) - processes inside the container can communicate via system-level IPC only to processes inside the same container.</li> <li><code>pid</code> (Process ID) - processes inside the container are only able to see other processes inside the same container or inside the same pid namespace.</li> <li><code>net</code> (Network) - the container gets its own network stack.</li> </ul> <p>With pods, some of the above namespaces (<code>ipc</code>, <code>uts</code>, <code>net</code>) are shared between the containers, which weakens their isolation to allow for easier communication. More namespaces can be shared on-demand.</p> <p>\u2b95 Learn more: Containers vs. Pods - Taking a Deeper Look</p> <p>This means we could run another container inside the <code>rickroller</code> pod and access it using localhost. Here is an example (run <code>kubectl delete pod rickroller</code> first, then use <code>kubectl apply -f</code>): <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\nname: rickroller\nspec:\ncontainers:\n- name: sh\nimage: busybox\ncommand: ['sh', '-c', 'while 1; do sleep 10; done']\n- name: rickroller\nimage: derlin/rickroller:latest\nports:\n- containerPort: 8080\n</code></pre></p> <p>Now, let's get the index from the busybox container: <pre><code>kubectl exec rickroller --container sh -- wget localhost:8080 -O -\n</code></pre></p> <p>As you can see, we can communicate with the rickroller container from the sh container on localhost!</p> <p>Don't forget to remove the pod: <code>kubectl delete pod rickroller</code>.</p> Another sidecar example <p>Sidecars are way more interesting when it comes to e.g. providing authentication, exposing logs, etc. Here is another example, this time with a shared volume: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\nname: example\nlabels:\napp: webapp\nspec:\ncontainers:\n- name: app\nimage: nginx\nports:\n- containerPort: 80\nvolumeMounts:\n- name: shared-logs\nmountPath: /var/log/nginx\n- name: sidecar\nimage: busybox\ncommand: [\"sh\", \"-c\", \"touch /var/log/nginx/access.log; tail -F /var/log/nginx/access.log\"]\nvolumeMounts:\n- name: shared-logs\nmountPath: /var/log/nginx\nvolumes:\n- name: shared-logs\nemptyDir: {}\n</code></pre> Try it!</p>"},{"location":"01-deploy-raw/#deployments-statefulsets-replicatsets","title":"Deployments, StatefulSets, ReplicatSets","text":"<p>Pods are usually never created directly but managed with higher-level resources.</p> <p>There are two (three) main common abstractions that simplify the management of containerized applications. you define a desired state and let k8s take care of the details (create, scale, update, etc.) of the underlying containers and pods.</p> <code>Deployments</code> Deployments provide a declarative way to manage the creation and scaling of a set of identical pods. You provide a pod template, and it takes care of creating the necessary ReplicaSets that will in turn ensure the desired number of pods are running. <code>StatefulSets</code> StatefulSets are similar to Deployments, but are used for stateful applications that require stable network identities, persistent storage, and ordered deployment and scaling. For example, the name of the pods will always look the same (e.g. <code>&lt;name&gt;-0</code>) instead of having a random UUID. (<code>ReplicaSets</code>) ReplicaSets are used by <code>Deployments</code> and <code>StatefulSets</code> to manage the desired number of replicas (i.e., identical instances) of a pod template. Each replicaset is configured with a minimum, maximum, and desired number of replicas, and will do all the necessary actions to ensure those values are respected. You usually don't create them directly but know they are here. <p>In the case of rickroller, the web app should be handled by a <code>Deployment</code>, while a database will likely use a <code>StatefulSet</code> as it requires persistent storage.</p>"},{"location":"01-deploy-raw/#using-a-deployment","title":"Using a Deployment","text":"<p>Deployments can be created using YAML or directly through kubectl. Via kubectl:</p> <pre><code>kubectl create deployment rickroller --image=derlin/rickroller:latest --port=8080 -o yaml # --dry-run=client \n</code></pre> <p>Or using a yaml file: kube/depl.yaml<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: rickroller\nspec:\nreplicas: 1 # number of desired pods\nselector:\nmatchLabels: # &lt;-\napp: rickroller\ntemplate: # template used to create the ReplicaSet the pods\nmetadata:\nlabels:\napp: rickroller # &lt;-\nspec:\ncontainers:\n- image: derlin/rickroller:latest\nname: rickroller\nports:\n- containerPort: 8080\n</code></pre></p> <p>Note the <code>spec.selector.matchLabels</code> and the <code>spec.template.metadata.labels</code>. The selector is used to determine which pods are part of this deployment, and thus the number of instances running.</p> <p>By creating the deployment, we should see 1 pod, 1 replicaset and 1 deployment:</p> <p><pre><code>kubectl get all\n</code></pre> <pre><code>NAME                             READY   STATUS    RESTARTS   AGE\npod/rickroller-dd4b47459-j9mdg   1/1     Running   0          25s\n\nNAME                         READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/rickroller   1/1     1            1           25s\n\nNAME                                   DESIRED   CURRENT   READY   AGE\nreplicaset.apps/rickroller-dd4b47459   1         1         1       25s\n</code></pre></p> <p>Now, let's see how useful a deployment is. First, let's say there is huge traffic and we need to scale horizontally. For this, run: <pre><code>kubectl scale deployment rickroller --replicas=3 # or 100, or 0 :)\n</code></pre> You can achieve the same by editing the deployment directly and changing the <code>spec.replicaCount</code> (using e.g. <code>kubectl edit</code> or <code>kubectl apply</code>).</p> <p>Let's also test fault tolerance. Delete one or more pods and see what happens. For example: <pre><code>kubectl delete $(kubectl get pod -o name | grep rickroller | head -1)\n</code></pre> A new pod is spawned immediately to take its place. Note that you can use <code>kubectl get pod -w</code> on a terminal if you do not have a UI (<code>-w</code> is for <code>--watch</code>).</p> <p>Now, we need to perform an upgrade. First, ensure you have either the UI opened to the pods view, or type <code>kubectl get pod -w</code> on a new terminal window. We will simulate an update by adding an environment variable to the pods called <code>BEHIND_PROXY</code> (this tells rickroller to honor the <code>X-Forwarded-*</code> headers, if any, which will be useful later): <pre><code>kubectl edit deploy rickroller\n## under .spec.template.spec.containers[0], add the following:\n##   image: ...\n##   env:\n##     - name: BEHIND_PROXY\n##       value: \"true\"\n## then save and exit (:wq on vim)\n</code></pre></p> <p>Or simply use the updated YAML found in <code>kube/env-depl.yaml</code> (ensure you have the <code>spec.replicaCount</code> set to 3)!</p> <p>When a Deployment is updated, the following steps happen:</p> <ol> <li>Kubernetes creates a new ReplicaSet with the updated pod template.</li> <li>Kubernetes starts scaling down the old ReplicaSet and scaling up the new ReplicaSet,    replacing pods one at a time. Only when the new pod is running and ready will it start receiving    traffic and the next pod will be upgraded. This is called the rolling upgrade strategy, which is the    default for <code>Deployment</code> (see <code>.spec.strategy.type</code>).</li> <li>the old ReplicaSet (scaled to 0 pods) stays around to allow for rollbacks in case of issues.</li> </ol> <p>This process is designed to ensure that the update is rolled out gradually, with minimal disruption to the application,  and that the old version of the application can easily be rolled back if necessary.</p> <p>Finally, let's say the upgrade didn't go well and we need to roll back to the previous version. This time, look at the replicasets and run: <pre><code>kubectl rollout undo deployment/rickroller\n</code></pre></p> <p>Finally, look at the history by typing: <pre><code>kubectl rollout history deployment/rickroller\n</code></pre></p> <p>Cool, right?</p> The pod-template-hash label <p>If you inspect the replicatset, you should see something like: <pre><code>spec:\nselector:\nmatchLabels:\napp: rickroller\npod-template-hash: dd4b47459  # &lt;- ?\n</code></pre> This pod template hash is also found in the name of the pods: <code>&lt;name&gt;-&lt;pod-hash&gt;-&lt;random&gt;</code>. This label is what allows multiple replicaset to run for the same application.</p>"},{"location":"01-deploy-raw/#services","title":"Services","text":"<p>Now that we have our pod running and fault tolerant, we need a way to communicate with rickroller in a stable manner. With Deployments and ReplicaSets, the pods come and go, so we cannot use their IP address or hostname. How? By using a <code>Service</code>.</p> <p>A service can be thought of as a receptionist at the front desk of a building. Just like a receptionist directs visitors to different departments in the building, a Kubernetes service directs network traffic to different pods in a cluster. It acts as a single entry point to access a group of pods providing the same service and ensures that traffic is evenly distributed among them. The way it directs traffic is by default round-robin, but can be customized to use session affinity or other means.</p> <p>There are three types of services:</p> ClusterIP This is the default type of service and provides a stable IP address that can be used to access a set of pods within the same cluster. It is used for internal communication within the cluster. NodePort This type of service exposes the service on a static port on each node in the cluster, allowing external traffic to access the service. It is useful for testing and development purposes. LoadBalancer This type of service exposes the service on a load balancer in cloud environments such as AWS or GCP, allowing external traffic to access the service. It is useful for production environments where high availability is required. <p>Let's create a service for the rickroller application. Since Exoscale provides support for load balancers, we can use the <code>loadbalancer</code> type - note  that it will take a minute, as Exoscale needs to provision a brand new load balancer (look at the Exoscale console): <pre><code>kubectl create service loadbalancer rickroller --tcp=80:8080 # -o yaml --dry-run=client\n</code></pre></p> <p>Or use the YAML: kube/lb-svc.yaml<pre><code>apiVersion: v1\nkind: Service\nmetadata:\nname: rickroller\nspec:\ntype: LoadBalancer\n# \u2193 to which pod should we redirect traffic\nselector:\napp: rickroller\nports:\n- port: 80  # service port\ntargetPort: 8080  # port of the container\n</code></pre></p> <p>To get the load balancer IP (that you can use in a browser), run the following and look for LoadBalancer Ingress: <pre><code>kubectl describe svc rickroller # svc is short for service\n</code></pre></p> <p>To see to which pods the service redirects traffic, use: <pre><code>kubectl get endpoints rickroller\n</code></pre></p>"},{"location":"01-deploy-raw/#ingresses","title":"Ingresses","text":"<p>When we set up the cluster, I said we installed the NGINX ingress controller. This lets us use only one cloud load balancer for multiple applications thanks to ingresses.</p> <p>Ingresses are a way to map a host or a route (e.g. a prefix) to a given service or app. The ingress controller is the component behind the load balancer, which is responsible for reacting to those ingresses, and for configuring networking properly. Without more details, remember that ingresses are great, but do nothing if you haven't installed at least one ingress controller.</p> <p>To use an ingress, let's change our service above to use a cluster IP instead of a load balancer: <pre><code>kubectl delete svc rickroller #\u00a0delete the old service (may take some time)\n# use the command below, or change LoadBalancer -&gt; ClusterIP in the YAML above (see kube/svc.yaml)\nkubectl create svc clusterip rickroller --tcp=80:8080\n</code></pre></p> <p>Here is how to create an ingress for rickroller, assuming the Nginx ingress controller is used: <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\nname: rickroller\nannotations:\n# Tell k8s to use the ingress controller\nkubernetes.io/ingress.class: \"nginx\"\n# \u2193 Annotations to configure nginx\nnginx.ingress.kubernetes.io/use-regex: \"true\"\nnginx.ingress.kubernetes.io/rewrite-target: /$2\nnginx.ingress.kubernetes.io/x-forwarded-prefix: /rickroller\nspec:\nrules:\n- http:\npaths:\n- path: /rickroller(/|$)(.*)\npathType: Prefix\nbackend:\nservice:\nname: rickroller\nport: # this is the port of the ClusterIP service\nnumber: 80\n</code></pre></p> <p>The above ingress tells the Nginx controller to direct incoming traffic with path <code>/rickroller</code> to the service rickroller on port 80. Moreover, the annotations ask Nginx to strip the prefix when forwarding and to set the <code>X-Forwarded-*</code> annotations.</p> <p>Nginx Ingress Controller Overview</p> <p></p> <p>The figure above shows how it works with the SKS cluster (Exoscale) and the Nginx Ingress Controller.</p> <p>The Nginx Ingress Controller defines a <code>Service</code> of type <code>LoadBalancer</code>, and thus receives an external IP from Exoscale. It then routes to one of the running Nginx pods running the Nginx reverse-proxy. The Nginx configuration (viewable in the pods at <code>/etc/nginx/nginx.conf</code>) is updated every time something happens with an Ingress. In our case, we create an ingress that asks Nginx to route the <code>/rickroller</code> prefix to the rickroller service, which knows how to forward it to one of the rickroller pods.</p> <p>The user can thus type <code>http://&lt;load-balancer-ip&gt;/rickroller</code> in the browser to get to the rickroller app!</p> Have a look at the Nginx configuration <p>If you are curious, you can spawn a shell inside one of the <code>ingress-nginx-controller-xxx</code> pods running in the <code>ingress-nginx</code> namespace, and look into the <code>/etc/nginx/nginx.conf</code>: <pre><code>kubectl -n ingress-nginx exec $(kubectl -n ingress-nginx get pod -o name | grep controller | head -1) -- \\\ncat /etc/nginx/nginx.conf | grep -a6 rickroller\n\n    location ~* \"^/rickroller\" {\n\nset $namespace      \"test\";\nset $ingress_name   \"roller\";\nset $service_name   \"roller\";\nset $service_port   \"80\";\nset $location_path  \"/rickroller\";\nset $global_rate_limit_exceeding n;\n\nrewrite_by_lua_block {\nlua_ingress.rewrite({\nforce_ssl_redirect = false,\n                ssl_redirect = true,\n    --\n\n        # In case of errors try the next upstream server before returning an error\nproxy_next_upstream                     error timeout;\nproxy_next_upstream_timeout             0;\nproxy_next_upstream_tries               3;\n\nrewrite \"(?i)/rickroller\" /$2 break;\nproxy_set_header X-Forwarded-Prefix \"/rickroller\";\nproxy_pass http://upstream_balancer;\n\nproxy_redirect                          off;\n\n}\n</code></pre> That's it! A simple Nginx reverse-proxy with a dynamic configuration.</p> <p>The application needs to support residing behind a reverse proxy (it receives traffic under <code>/</code>, but is reachable under <code>/rickroller</code>). This is the case of rickroller, given we tell him to by setting the <code>BEHIND_PROXY</code> environment variable. To do so, ensure you are using the <code>kube/depl-update.yaml</code> definition:</p> kube/env-depl.yaml<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: rickroller\nspec:\nreplicas: 3 # number of desired pods\nselector:\nmatchLabels: # &lt;-\napp: rickroller\ntemplate: # template used to create the ReplicaSet the pods\nmetadata:\nlabels:\napp: rickroller # &lt;-\nspec:\ncontainers:\n- image: derlin/rickroller:latest\nname: rickroller\nports:\n- containerPort: 8080\nenv:\n- name: BEHIND_PROXY\nvalue: \"true\"\n</code></pre> <p>Note</p> <p>Environment variables can also be added through <code>ConfigMaps</code>, but I will let you discover this by yourself.</p>"},{"location":"01-deploy-raw/#recap","title":"Recap'","text":"<p>Kubernetes Pods are like regular containers but can have one or more containers sharing the same network stack, hostname, and memory. We never directly deploy pods, but use higher levels abstractions such as Deployments for stateless applications, and StatefulSets for stateful apps. Deployments and StatefulSets use <code>ReplicaSet</code> under the hood to scale pods up and down, manage rolling upgrades, and implement fault tolerance.</p> <p>Since an application can run on multiple pods, and the latter are ephemeral (they can be killed, moved to other nodes, etc), we use a Service to provide a single point of entry (IP address and hostname). The service constantly tracks the pods part of the same application and knows how to redirect traffic. Services are usually used internally, but they can be exposed to the outside using a load balancer or a node port.</p> <p>Finally, a typical way of accessing applications from the outside is to create an Ingress. Ingresses are special constructs that do nothing until an ingress controller is installed on the cluster. In our example, we used the Nginx ingress controller, which has a load balancer IP and redirects traffic to internal services using Nginx. Whenever an ingress is created or updated, the controller updates the Nginx configuration accordingly. The usual features of Nginx are available, with routing based on hostnames, path prefixes, etc.</p>"},{"location":"02-helm/","title":"Helm","text":"<p>Now that rickroller is deployed, how would you:</p> <ul> <li>rename all resources to <code>roller</code> instead of <code>rickroller</code>?</li> <li>add labels to all resources to have <code>author=lucy</code>?</li> <li>delete the whole rickroller application?</li> <li>give the app to a friend so that he can deploy it on his Kubernetes cluster, which potentially   doesn't use the same version, ingress controller, etc.?</li> </ul> <p>With YAML files, there is no magic: we should go through all, make modifications, and re-apply all of them one by one. To delete the app, we should go through many <code>kubectl delete</code> and ensure we do not forget one. Everything is tedious: there is no logical grouping, no customization, no code reuse, nothing.</p> <p>Enter Helm!</p>"},{"location":"02-helm/#what-is-helm","title":"What is Helm?","text":"<p>Helm is a package **manager for Kubernetes**. From Helm homepage:</p> <p>Helm helps you manage Kubernetes applications \u2014 Helm Charts help you define, install, and upgrade even the most complex Kubernetes application. Charts are easy to create, version, share, and publish \u2014 so start using Helm and stop the copy-and-paste.</p>"},{"location":"02-helm/#helm-chart-basics","title":"Helm Chart basics","text":"<p>To get started with Helm, create a new directory with the following structure: <pre><code>example\n\u251c\u2500\u2500 Chart.yaml   # metadata\n\u251c\u2500\u2500 values.yaml  # parameters\n\u2514\u2500\u2500 templates    # templates\n\u2514\u2500\u2500 ...\n</code></pre></p> <p>Let's add the following content: Chart.yaml<pre><code>apiVersion: v2 # Helm version\nname: example\ndescription: basic example chart version: 0.1.0\n</code></pre></p> templates/example.yaml<pre><code>some: yaml\n</code></pre> <p>Now, the directory has become a helm chart. Let's see how it works by running the following at the root of the chart directory: <pre><code>helm template .\n---\n# Source: example/templates/example.yaml\nsome: yaml\n</code></pre></p> <p>Okay, so a Helm chart outputs the content of the files inside the <code>templates</code> directory. So why all the fuss? Well, the power of Helm comes from its templating capabilities based on Go template. Helm has over 60 available functions. Some of them are defined by the Go template language itself, and most of the others are part of the Sprig template library.</p> <p>Inside a go template, we have access to multiple contextual information coming from different sources. Let's see it in action.</p> <p>Replace your template with the following content: templates/example.yaml<pre><code># \".\" is the root context, {{ }} denotes a go template expression\n{{ . | toYaml }}\n</code></pre></p> <p>Let's template the chart again: <code>helm template .</code>. You should see: <pre><code>---\n# Source: example/templates/example.yaml\nCapabilities:\nAPIVersions:\n- v1\n...\nHelmVersion:\ngit_commit: 912ebc1cd10d38d340f048efaf0abda047c3468e\ngit_tree_state: clean\ngo_version: go1.20.2\nversion: v3.11.2\nKubeVersion:\nMajor: \"1\"\nMinor: \"26\"\nVersion: v1.26.0\nChart:\nIsRoot: true\napiVersion: v2\ndescription: basic example chart\nname: example\nversion: 0.1.0\nFiles: {}\nRelease:\nIsInstall: true\nIsUpgrade: false\nName: release-name\nNamespace: test\nRevision: 1\nService: Helm\nSubcharts: {}\nTemplate:\nBasePath: example/templates\nName: example/templates/example.yaml\nValues: {}\n</code></pre></p> <p>To break it down, this is everything you can query from a go template:</p> <ul> <li><code>.Capabilities</code>: the API resources available in the cluster you are currently connected to.   This let's you customize your output depending on the k8s cluster (using <code>if/else</code> etc.)</li> <li><code>.Chart</code>: information from your <code>Chart.yaml</code> file, including the current version of the chart, etc.</li> <li><code>.Files</code>: additional files in the helm chart directory, that you could read and use in your templates</li> <li><code>.Release</code>: the name, namespace etc. of the helm chart as it is being installed</li> <li><code>.Subcharts</code>: helm charts can include other charts (we will talk about this later)</li> <li><code>.Template</code>: template files information</li> <li><code>.Values</code>: content of <code>values.yaml</code></li> </ul> <p>The most useful are <code>.Release</code> and <code>.Values</code>. Whatever you put in your <code>values.yaml</code> will be available in the templates. They act as the parameters to the templates and let you customize the helm chart at install time. </p> <p>To better understand the <code>.Values</code>, add this: values.yaml<pre><code>hello: world\n</code></pre> Now, run the following two commands: <pre><code>helm template . | tail -2\n</code></pre> <pre><code>Values:\nhello: world\n</code></pre> <pre><code>helm template . --set hello='Fribourg!' | tail -2\n</code></pre> <pre><code>Values:\nhello: Fribourg!\n</code></pre></p> <p>Helm charts are thus a bunch of templates that should output valid Kubernetes Manifests files (as YAML) based on the parameters initially defined in <code>values.yaml</code> but overridable from the command line.</p> <p>Valus are overridable from the command line using <code>--set</code>, or from a YAML file using <code>--values &lt;file&gt;</code>. The content of the provided file will be merged with the content of the <code>values.yaml</code>, with precedence to the former.</p> <p>Be careful! YAML merge works great with dictionaries, but not with lists: list are completely replaced, the items are not merged.</p> A more accurate example <p>To have a valid helm chart, you could replace the content of <code>templates/example.yaml</code> with the following: <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: {{ .Release.Name }}\nspec:\nreplicas: {{ .Values.replicas | default 1 }}\nselector:\nmatchLabels: app: {{ .Release.Name }}\ntemplate:\nmetadata:\nlabels:\napp: {{ .Release.Name }}\nspec:\ncontainers:\n- image: derlin/rickroller:{{ .Values.tag | default \"latest\" }}\nname: {{ .Release.Name }}\nports:\n- containerPort: 8080\n</code></pre></p> <p>You can now play with it, setting e.g. the values <code>--set replicas=2</code> or <code>--set tag=1.0.0</code>.</p>"},{"location":"02-helm/#working-with-a-helm-chart","title":"Working with a Helm Chart","text":"<p>Of course, real helm charts are quite complex and necessitate getting acquainted with the go template syntax. To get started, you can try using the <code>helm create</code> function. For the next examples, I already created a Helm chart (using <code>helm create</code> as a base) for rickroller that you can find in the <code>helm/rickroller</code> directory. Assume all the next commands are run from there.</p> <p>Tip</p> <p>I suggest you create a new namespace for your tests with Helm.</p> <p>Now that rickroller is packaged with helm, we can install it in our cluster using: <pre><code>helm install roller .\n</code></pre> <pre><code>Release \"roller\" does not exist. Installing it now.\nNAME: roller\nLAST DEPLOYED: Mon Apr 17 15:43:16 2023\nNAMESPACE: example\nSTATUS: deployed\nREVISION: 1\nNOTES:\n1. Get the application URL by running these commands:\n  http:///roller(/|$)(.*)\n</code></pre></p> <p>Now, you can run the following:</p> <ul> <li><code>helm list</code> \u2192 roller is installed</li> <li><code>helm upgrade roller . --set replicas=2</code> \u2192 scale to 2</li> <li><code>helm rollback roller</code> \u2192 we are back to the first revision with one replica</li> <li><code>helm history roller</code> \u2192 we have 3 revisions: one install, one upgrade, and a rollback</li> <li><code>helm uninstall roller</code> \u2192 completely uninstalls everything.</li> </ul> <p>You want to not use an ingress, but a service of type <code>LoadBalancer</code> (as we did previously) instead? Easy: <pre><code>helm upgrade --install roller . --set ingress.enabled=false --set service.type=LoadBalancer\n</code></pre></p>"},{"location":"02-helm/#using-charts-from-a-repository","title":"Using charts from a repository","text":"<p>So far, we used a local chart. Let's use helm now to deploy MongDB from bitnami. First, add the repo so your machine knows how to fetch bitnami charts:</p> <pre><code>helm repo add bitnami https://charts.bitnami.com/bitnami\n</code></pre> <p>Now, install mongodb with an initial database, username and password: <pre><code>helm upgrade --install mongodb bitnami/mongodb --version 13.9.1 \\\n--set architecture=standalone \\\n--set 'auth.usernames[0]=myuser' --set 'auth.passwords[0]=mypass' --set 'auth.databases[0]=mydb' \\\n--set persistence.enabled=false\n</code></pre></p> Using persistence <p>Since we installed longhorn, we can turn on persistence if we want to: <pre><code>helm upgrade --install mongodb bitnami/mongodb --version 13.9.1 \\\n--set architecture=standalone --set 'auth.usernames[0]=myuser' --set 'auth.passwords[0]=mypass' --set 'auth.databases[0]=mydb' \\\n--set persistence.enabled=true --set persistence.storageClass=longhorn --set persistence.size=1Gi\n</code></pre></p> <p>We can now deploy/upgrade rickroller and ask it to use mongodb: <pre><code>helm upgrade --install rickroller . --set env.DATABASE_URL='mongodb://myuser:mypass@mongodb/mydb'\n</code></pre></p>"},{"location":"02-helm/#using-subcharts","title":"Using subcharts","text":"<p>It is also possible to make mongodb part of the rickroller Helm chart. The advantage is that we only need one helm command to get both. The disadvantage? They are treated as a whole: we cannot update or uninstall one without the other.</p> Adding mongodb as a subchart <p>First, add the following to <code>Chart.yaml</code>: <pre><code>dependencies:\n- name: mongodb\nversion: 13.9.1\nrepository: \"@bitnami\"\n</code></pre></p> <p>Next, add the following to <code>values.yaml</code>: <pre><code>mongodb:\nauth:\nrootPassword: root\nusernames: [myuser]\npasswords: [mypass]\ndatabases: [mydb]\npersistence:\nenabled: false\nstorageClass: longhorn\nsize: 1Gi\n\n# ...\n\nenv:\n# ...\nDATABASE_URL: mongodb://myuser:mypass@{{ .Release.Name }}-mongodb/mydb\n</code></pre></p> <p>Done! Run the following command once and you are good to go: <pre><code># update the dependencies\n# this will create a folder charts/ with a tar of the mongodb chart\nhelm dependency update\n</code></pre></p> <p>A chart can have any number of subcharts, making it possible to treat a large and complex project as a whole. In this case, we call the root helm chart an umbrella chart.</p> <p>The umbrella chart is responsible for creating the namespace and the global components such as the network policies, and each microservice is included as a dependency.</p> <p>It is even possible to reuse the same generic helm chart for multiple dependencies/microservice. For this, we only have one subchart directory (only one chart under <code>charts</code>), but reference it multiple times in the dependencies. This generic subchart is then configured with different values depending on the microservice.</p> <p>Here is an example:</p> Chart.yaml<pre><code>apiVersion: v2\nname: my-umbrella-chart\ndescription: A Helm umbrella chart for deploying my application.\ntype: umbrella\nversion: 1.0.0\n\n# the name reference the chart (templates),\n# the alias is the name we give for this instance of the templates\ndependencies:\n- alias: service-1\nname: generic-chart\nversion: 1.0.0\ncondition: service-1.enabled # we can even use conditions!\n\n- alias: service-N\nname: generic-chart\nversion: 1.0.0\ncondition: service-N.enabled\n</code></pre> values.yaml<pre><code>service-1:\nenabled: true\nport: 8080\nenv:\nSERVICE_1_CONTEXT: bar\n# ...\n\nservice-N:\nenabled: true\nport: 80\ningress:\nenabled: true\nhost: service-n.example.com\n# ...\n</code></pre>"},{"location":"03-helmfile/","title":"Helmfile","text":"<p>Usually, an application is composed of many microservices, each of them packaged as a helm chart. When you want to install or upgrade the application as a whole, how would you proceed? Running 20+ <code>helm upgrade</code> doesn't seem like a very interesting job. Moreover, charts may:</p> <ul> <li>have dependencies on others,</li> <li>share some common configurations,</li> <li>require a different configuration depending on the environment (demo, prod, etc.),</li> <li>need to be installed in a specific order (a service may need a database to be running first to start properly).</li> </ul> <p>Enter helmfile!</p>"},{"location":"03-helmfile/#what-is-helmfile","title":"What is Helmfile","text":"<p>Warning</p> <p>Even though Helmfile is used in production environments across multiple organizations, it is still in its early stage of development, hence versioned 0.x.</p> <p>helmfile is a declarative spec for deploying helm charts. With it you can:</p> <ul> <li>declare all the charts and values part of an application in one place (e.g. a git repo!)</li> <li>sync it periodically using CI to avoid skewed environments,</li> <li>have multiple environments, with different configurations,</li> <li>select/filter releases (very handy for debugging and development),</li> <li>declare common values applied to all charts,</li> <li>declare an order in which helm charts are installed1,</li> <li>only upgrade charts that changed (thanks to helm-diff),</li> <li>etc.</li> </ul> <p>Using helmfile requires a bit of practice, especially when it comes to values 2, but I came to love it anyhow.</p>"},{"location":"03-helmfile/#deploying-with-helmfile","title":"Deploying with helmfile","text":"<p>Declaring our rickroller application using helmfile comes down to the following: helmfile.yaml<pre><code>releases:\n- name: rickroller\nchart: ../helm/rickroller\nneeds: [mongodb]\nvalues:\n- replicas: 1\nenv:\nDATABASE_URL: mongodb://myuser:mypass@mongodb/mydb\n\n- name: mongodb\nchart: bitnami/mongodb\nversion: 13.9.1\nvalues:\n- auth:\nrootPassword: root\nusernames: [myuser]\npasswords: [mypass]\ndatabases: [mydb]\npersistence:\nenabled: false\nstorageClass: longhorn\nsize: 1Gi\n\nrepositories:\n- name: bitnami\nurl: https://charts.bitnami.com/bitnami\n</code></pre></p> <p>Now that we have declaratively defined our app, we can run:</p> <ul> <li><code>helmfile list</code> \u2192 list all the releases. Note that the installed column means \"it will be installed, not \"it is already present in the cluster\"</li> <li><code>helmfile template</code> \u2192 run <code>helm template</code> on all releases, useful for debugging</li> <li><code>helmfile write-values</code> \u2192 compute and print the values that will be passed to each helm release, useful for debugging</li> <li><code>helmfile sync</code> \u2192 run <code>helm upgrade --install</code> on all releases</li> <li><code>helmfile apply</code> \u2192 look at what is already present in the cluster, and run <code>helm upgrade --install</code> only on releases that changed</li> <li><code>helmfile diff</code> \u2192 only run the diff</li> <li><code>helmfile destroy</code> \u2192 uninstall all releases</li> </ul> <p>One of the things I use most is the ability to select specific releases when running commands:</p> <p><code>-l</code>, <code>--selector stringArray</code></p> <p>Only run using the releases that match labels. Labels can take the form of <code>foo=bar</code> or <code>foo!=bar</code>. A release must match all labels in a group in order to be used. Multiple groups can be specified at once. <code>\"--selector tier=frontend,tier!=proxy --selector tier=backend\"</code> will match all frontend, non-proxy releases AND all backend releases. The name of a release can be used as a label: <code>\"--selector name=myrelease\"</code></p> <p>In our example, we could thus only template the rickroller release using: <pre><code>helmfile -l name=rickroller template\n</code></pre></p> <p>Warning</p> <p>The labels here refer to helmfile labels that can be added to any release in the state file. They have nothing to do with Kubernetes <code>.metadata.labels</code>, and are only used by internally helmfile.</p> <p>This ability makes it easier for developers to work with big helmfiles, and is a big advantage against an umbrella chart. As we discussed in the last chapter, umbrella charts are another way to pack many different helm charts into one project. There is, however, no easy way to work on only one release with umbrella chart, except by using <code>conditions</code> (which need to be all turned off, except for the one we are interested in).</p>"},{"location":"03-helmfile/#using-environments","title":"Using environments","text":"<p>There are many other features of helmfile, for example, the ability to compose state files, define environments, etc.</p> <p>In our example, let's say we have three environments:</p> <ul> <li><code>sandbox</code> (default) \u2192 we only want to deploy rickroller without any database</li> <li><code>dev</code> \u2192 we want to deploy mongodb as well, but without any persistence (no StatefulSet)</li> <li><code>prod</code> \u2192 we need mongodb to use persistence, it is production!</li> </ul> <p>Here is the helmfile we could use (see <code>helmfile/helmfile-env.yaml</code>):</p> <pre><code>environments:\n# we will use the default environment as our sandbox\ndefault:\ndev:\nprod:\n# we can also have values, which will be available\n# in helmfile templates as .Values\nvalues:\n- mongoPassword: very-strong-password\n\n---\n\nreleases:\n- name: rickroller\nchart: ../helm/rickroller\nneeds: [mongodb]\nvalues:\n- replicas: 1\nenv:\n# pass an empty database url in the default environment\nDATABASE_URL: {{ ternary \"\" \"mongodb://myuser:mypass@mongodb/mydb\" \n            (eq .Environment.Name \"default\")  }}\n\n- name: mongodb\nchart: bitnami/mongodb\nversion: 13.9.1\n# Only install mongodb in non-default environment\ninstalled: {{ ne .Environment.Name \"default\" }}\nvalues:\n- auth:\nrootPassword: root\nusernames: [myuser]\npasswords:\n# read the password from environment values, if provided\n- {{ .Values | get \"mongoPassword\" \"mypass\" }}\ndatabases: [mydb]\npersistence:\n# enable persistence for prod only\nenabled: {{ eq .Environment.Name \"prod\" }}\nstorageClass: longhorn\nsize: 1Gi\n\nrepositories:\n- name: bitnami\nurl: https://charts.bitnami.com/bitnami\n</code></pre> <p>Warning</p> <p>The go templates we are using are interpreted and rendered by helmfile, they are not passed to helm! Thus, the <code>.Values</code> are not our usual values, they hold the <code>environments.values</code> defined in the helmfile <code>environments</code> section. This is quite confusing at first. So remember: we have two layers of go templates!</p> <p>To pass a template to the helm chart, we would have to write something like this: <pre><code>valueAcceptingTemplate: {{ `{{ .Release.Name }}` }}\n</code></pre></p> <p>You can now deploy different flavors using the <code>-e &lt;env&gt;</code> flag. For example: <pre><code>helmfile -e prod sync\n</code></pre></p> <ol> <li> <p>See my article helmfile: understand (and visualize !) the order in which releases are deployed for more details.\u00a0\u21a9</p> </li> <li> <p>See my article helmfile: a simple trick to handle values intuitively \u21a9</p> </li> </ol>"},{"location":"04-argo/","title":"Argo CD","text":"<p>We now have an easy way to deploy and update a complex project using helm and helmfile but we still have to perform these actions manually! How can we automate this and follow GitOps principles? Enter Argo CD, a declarative, GitOps continuous delivery tool for Kubernetes.</p> <p></p> <p>Argo CD follows the GitOps pattern of using Git repositories as the source of truth for defining the desired application state. Kubernetes manifests can be specified in several ways: kustomize applications, helm charts, plain YAML manifests, and more. It then automates the deployment of the desired application states in the specified target environments by watching the changes in your git repo (via webhooks) and applying them in a continuous, self-healing manner.</p> <p>Application deployments can track updates to branches, tags, or pinned to a specific version of manifests at a Git commit.</p> <p>Argo CD needs to be installed on a Kubernetes cluster, and can from there drive other clusters (multi-clusters support). This is a very powerful pattern that is used by many organizations to manage complex infrastructures.</p> <p>Note</p> <p>Another great contender of Argo CD is Flux CD.</p>"},{"location":"04-argo/#argo-cd-and-helmfile","title":"Argo CD and helmfile","text":"<p>There is sadly no built-in support for helmfile, despite an open issue from August 2019.</p> <p>One way to keep using helmfile is to have a CI workflow that uses <code>helmfile template</code> and saves the output as raw YAML manifests that can be later used by Argo CD. Another way is to use plugins. I found and tested two of them:</p> <ul> <li>travisghansen/argo-cd-helmfile: this plugin is referenced directly in the open issue.   It works well and has tons of options, but there is currently a bug in the Argo CD interface that prevents you from creating an <code>AppProject</code>   directly from the UI using helmfile. All works great if you use the argocd CLI though.</li> <li>lucj/argocd-helmfile-plugin: this plugin is fairly basic but works.</li> </ul>"},{"location":"04-argo/#installing-argo-cd","title":"Installing Argo CD","text":"<p>Argo CD can be installed using helm or plain Manifests. Since we are using plugins and are now familiar with helmfile, let's install it this way:</p> argo-cd/install/helmfile.yaml<pre><code>repositories:\n- name: argo\nurl: https://argoproj.github.io/argo-helm\n\nreleases:\n- name: argocd\nnamespace: argocd\nlabels:\napp: argocd\nchart: argo/argo-cd\nversion: ~5.28.2\nvalues:\n- repoServer:\nextraContainers:\n- name: plugin\nimage: lucj/argocd-plugin-helmfile:v0.0.11\ncommand: [\"/var/run/argocd/argocd-cmp-server\"]\nsecurityContext:\nrunAsNonRoot: true\nrunAsUser: 999\nvolumeMounts:\n- mountPath: /var/run/argocd\nname: var-files\n- mountPath: /home/argocd/cmp-server/plugins\nname: plugins\n</code></pre> <p>Install it using: <pre><code>helmfile -f argo-cd/install/helmfile.yaml sync </code></pre></p> <p>Now that ArgoCD is installed, connect to the UI by creating a port-forward:</p> <pre><code>kubectl port-forward -n argocd svc/argocd-server 7000:80\n</code></pre> <p>ArgoCD is now available at https://localhost:7000.</p> <p>The initial <code>admin</code> password is stored in a secret called <code>argocd-initial-admin-secret</code> in the <code>argocd</code> namespace. You can get it easily by executing: <pre><code>kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d\n</code></pre></p> <p>Once you log in using the <code>admin</code> user and the password above, ensure you:</p> <ol> <li>delete the secret (<code>kubectl -n argocd delete secret argocd-initial-admin-secret</code>),</li> <li>change the admin password by clicking on the User Info in the left menu bar, then Update Password.</li> </ol>"},{"location":"04-argo/#basics-of-argo-cd","title":"Basics of Argo CD","text":"<p>Argo CD registers new Kubernetes Custom Resources (CRs) that define new resource kinds. Those can be managed using kubectl, as regular built-in resources.</p> <p>Argo CD has three constructs, defined in three different custom resource definitions or Kubernetes objects:</p> <ul> <li>An <code>AppProject</code> is a high-level grouping mechanism in Argo CD that allows defining access controls and configuration overrides for a set of related applications.</li> <li>An <code>Application</code> is a Kubernetes manifest and a set of parameters that define how a specific application should be deployed and managed by Argo CD.   It defines a source repository, the tool used (helm, etc), the destination cluster, etc.</li> <li>An <code>ApplicationSet</code> is a higher-level concept that allows defining multiple Applications with similar characteristics using templates, parameters and sources.   It enables scaling the number of Application instances with variations in parameters, labels, and annotations.   It can, for example, be used to spawn temporary environments when a pull request is created.</li> </ul> <p>Note that you can create an app that creates other apps, which in turn can create other apps. This allows you to declaratively manage a group of apps that can be deployed and configured in concert.</p> Viewing Custom Resources <p>If you are curious, you can see the content of a CRD (Custom Resource Definition) using: <pre><code>kubectl get crd | grep argo\nappprojects.argoproj.io                                 2023-04-18T12:27:11Z\napplications.argoproj.io                                2023-04-18T12:27:11Z\napplicationsets.argoproj.io                             2023-04-18T12:27:11Z\n</code></pre> <pre><code>kubectl get crd applications.argoproj.io -o yaml\n</code></pre></p> <p>You also query the instances of each crd using <code>kubectl get</code>. To discover the name you must use: <pre><code>kubectl api-resources | grep argo \napplications     app,apps          argoproj.io/v1alpha1  true  Application\napplicationsets  appset,appsets    argoproj.io/v1alpha1  true  ApplicationSet\nappprojects      appproj,appprojs  argoproj.io/v1alpha1  true  AppProject\n</code></pre> Hence, for applications, use <code>kubectl get apps</code>.</p> <p>To create one of those resources, we can:</p> <ul> <li>Use the Argo CD UI,</li> <li>Use the <code>argocd</code> CLI, or</li> <li>Use a YAML file and <code>kubectl apply</code>.</li> </ul>"},{"location":"04-argo/#the-app-project","title":"The App Project","text":"<p>By default, argocd creates a <code>default</code> App Project that has no restriction. Using <code>default</code>, you can deploy anything from and to anywhere.</p> How to see the default project definition <p>Use your Kubernetes knowledge!</p> <pre><code>kubectl get appprojects -n argocd default -o yaml\n</code></pre> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: AppProject\nmetadata:\ncreationTimestamp: \"2023-04-18T12:27:34Z\"\ngeneration: 1\nname: default\nnamespace: argocd\nresourceVersion: \"362197\"\nuid: 2c95d565-7b55-4f60-a253-002704b82b15\nspec:\nclusterResourceWhitelist:\n- group: '*'\nkind: '*'\ndestinations:\n- namespace: '*'\nserver: '*'\nsourceRepos:\n- '*'\nstatus: {}\n</code></pre> <p>Usually, we want to tighten security. For this demo, we will use the following <code>AppProject</code>. It creates a project called <code>demo</code> that limits the sources to one of my GitHub repositories (github.com/derlin) and only allows certain kinds of resources to be created. The two namespaces <code>kube-system</code> and <code>argocd</code> are also restricted: they cannot be used by an app as a destination.</p> argo-cd/appproject-demo.yaml<pre><code>apiVersion: argoproj.io/v1alpha1\nkind: AppProject\nmetadata:\nname: demo\nnamespace: argocd\n# Finalizer that ensures that project is not deleted until it is not referenced by any application\nfinalizers:\n- resources-finalizer.argocd.argoproj.io\nspec:\ndescription: Demo project to host rickroller\n# Allow manifests to deploy from any Git repos that are mine\nsourceRepos:\n- \"https://github.com/derlin/*\"\ndestinations:\n# Do not allow any app to be installed in `kube-system` or `argocd`\n- namespace: '!kube-system'\nserver: \"*\"\n- namespace: '!argocd'\nserver: '*'\n# Other destinations are fine though\n- namespace: '*'\nserver: '*'\n# Limit what resources an app is allowed to create\nclusterResourceWhitelist:\n- group: \"\"\nkind: Namespace\nnamespaceResourceWhitelist:\n- group: \"\"\nkind: Pod\n- group: apps\nkind: ReplicaSet\n- group: apps\nkind: Deployment\n- group: apps\nkind: StatefulSet\n- group: \"\"\nkind: Service\n- group: networking.k8s.io\nkind: Ingress\n- group: \"\"\nkind: ConfigMap\n- group: \"\"\nkind: Secret\n- group: \"\"\nkind: ServiceAccount\n- group: \"\"\nkind: Role\n- group: \"\"\nkind: RoleBinding\n- group: \"\"\nkind: PersistentVolume\n- group: \"\"\nkind: PersistentVolumeClaim\n</code></pre> <p>Create the project using <code>kubectl apply</code>. You should be able to now see the project in the Argo CD UI, under Settings &gt; Projects.</p>"},{"location":"04-argo/#the-app","title":"The App","text":"<p>Now that we have a project, we can create an app to deploy rickroller. Defining an app is quite straightforward:</p> argo-cd/app-rickroller.yaml<pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\nnamespace: argocd\nname: rickroller\n# Perform a cascading delete of the Application's resources when the App is deleted\nfinalizers:\n- resources-finalizer.argocd.argoproj.io\nspec:\nproject: demo # you could also use the \"default\" project if you want\nsource:\nrepoURL: https://github.com/derlin/fribourg-linux-seminar-k8s-deploy-like-a-pro\npath: helmfile       # directory of the helmfile.yaml\ntargetRevision: HEAD # use the default branch of the repo\ndestination:\nnamespace: rickroller\nserver: https://kubernetes.default.svc\nsyncPolicy:\n# Make Argo CD automatically apply changes without human intervention\nautomated:\nprune: true    # delete resources when they are no longer in git\nselfHeal: true # perform a sync when the cluster deviates from the state defined in Git (e.g. manual changes)\nsyncOptions:\n- CreateNamespace=true # required to also create namespace\n</code></pre> <p>Create the app using <code>kubectl apply</code>. You should now see the app under Applications. Watch how Argo CD syncs all resources and deploys rickroller automatically! Isn't it great?</p> <p>Using the argo CLI</p> <p>You can perform the same operations (and more, such as syncing) using the <code>argocd</code> CLI. As an example, here is how you could create the rickroller App from the terminal: argo-cd/rickroller-app.sh<pre><code>#!/usr/bin/env bash\n\n# This is equivalent to running kubectl apply -f app-rickroller.yaml\n# It is just an example on how to use the argocd CLI\nargocd app create rickroller \\\n--project demo \\\n--repo https://github.com/derlin/fribourg-linux-seminar-k8s-deploy-like-a-pro --path helmfile \\\n--dest-server https://kubernetes.default.svc --dest-namespace rickroller \\\n--set-finalizer \\\n--sync-option CreateNamespace=true \\\n--sync-policy automated --self-heal --auto-prune \\\n--upsert\n</code></pre></p>"},{"location":"04-argo/#what-now","title":"What now?","text":"<p>Now that we have our Argo CD set, we can commit and push any change and wait for the magic to happen. Since the app is configured with sync enabled, you should see your changes applied after around three minutes of committing. Why three minutes? Because Argo CD polls git repositories every three minutes to detect changes to the manifests.</p> <p>The automatic sync interval is determined by the <code>timeout.reconciliation</code> value in the <code>argocd-cm</code> ConfigMap, which defaults to 180s (3 minutes).</p> <p>If you want Argo CD to be more reactive, you will need to set up a webhook. This, of course, assumes your Argo CD instance is reachable from the outside.</p>"},{"location":"05-summary/","title":"Summary","text":"<p>In summary:</p> <ul> <li>Kubernetes Manifests are the basic way of deploying things to kubernetes<ul> <li>raw YAML files: no portability, no customisation, no code reuse</li> </ul> </li> <li>Use Helm to package manifests into installable units<ul> <li>powerful templating (Go Templates), rich context information and parameters (values.yaml)</li> <li>possible to publish Helm charts to registries (sharing)</li> <li>umbrella charts to group multiple charts under a parent one (sub-charts)</li> </ul> </li> <li>Use helmfile to group and manage multiple helm charts as one<ul> <li>you still have separate helm releases (vs umbrella chart)</li> <li>declarative YAML with many features (envs, templates)</li> <li>powerful CLI with tons of options</li> </ul> </li> <li>Use Argo CD to add GitOps and automation to the mix<ul> <li>automatically syncs the state defined in a git repo with one or more clusters</li> <li>supports raw manifests, kustomize, helm, jsonnet, \u2026 (+ helmfile through a plugin) </li> <li>and much more! multi-cluster support, ApplicationSets, etc.</li> </ul> </li> </ul> <p>I hope you enjoyed this tutorial. Please, leave a  on the github repo, it will help me stay motivated.</p>"}]}